{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune T5 locally for machine translation on COVID-19 Health Service Announcements with Hugging Face\n",
    "\n",
    "[reference](https://github.com/aws/studio-lab-examples/blob/main/natural-language-processing/NLP_Disaster_Recovery_Translation.ipynb)\n",
    "\n",
    "This notebook is designed to run within SageMaker Lab, on a `g4dn.xlarge` GPU instance. If you are not using that right now, please restart your session and select `GPU`, as this will help you train your model in a matter of tens of minutes, rather than hours.\n",
    "\n",
    "If you are ready for training a large-scale machine translation model, then please check out using Hugging Face on Amazon SageMaker! \n",
    "\n",
    "Otherwise, please enjoy this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Install all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "ipywidgets\n",
    "git+https://github.com/huggingface/transformers\n",
    "datasets\n",
    "sacrebleu\n",
    "torch\n",
    "sentencepiece\n",
    "mlfoundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 3))\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-znnldqlm\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-znnldqlm\n",
      "  Resolved https://github.com/huggingface/transformers to commit 49cd736a288a315d741e5c337790effa4c9fa689\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (7.7.0)\n",
      "Requirement already satisfied: datasets in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.3.2)\n",
      "Requirement already satisfied: sacrebleu in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.96)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (7.33.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (5.4.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (6.4.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (2022.6.2)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (1.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers==4.21.0.dev0->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: multiprocess in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (0.70.13)\n",
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (1.4.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (8.0.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (0.3.5.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (2022.5.0)\n",
      "Requirement already satisfied: xxhash in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (3.8.1)\n",
      "Requirement already satisfied: colorama in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.4.5)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.8.10)\n",
      "Requirement already satisfied: portalocker in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacrebleu->-r requirements.txt (line 5)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (4.2.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (0.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (7.3.0)\n",
      "Requirement already satisfied: backcall in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: pygments in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (2.12.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (62.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (3.0.29)\n",
      "Requirement already satisfied: pickleshare in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.7.5)\n",
      "Requirement already satisfied: decorator in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (22.3.0)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (4.9.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (1.5.5)\n",
      "Requirement already satisfied: fastjsonschema in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (2.15.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (4.5.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (21.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.21.0.dev0->-r requirements.txt (line 3)) (3.0.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers==4.21.0.dev0->-r requirements.txt (line 3)) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers==4.21.0.dev0->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers==4.21.0.dev0->-r requirements.txt (line 3)) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers==4.21.0.dev0->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (6.4.11)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: prometheus-client in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (6.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: defusedxml in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: bleach in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (5.0.0)\n",
      "Requirement already satisfied: tinycss2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (4.11.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2022.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# make sure to restart your kernel to use the newly install packages\n",
    "# IPython.Application.instance().kernel.do_shutdown(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Explore the available datasets on Translators without Borders \n",
    "Then, download a pair you would like to use for training a language translation model. The steps below download the translation pairs for English to Spanish, but you are welcome to modify these and use a different pair if you prefer.\n",
    "\n",
    "Overall site page: https://tico-19.github.io/\n",
    "\n",
    "Page with all language pairs: https://tico-19.github.io/memories.html \n",
    "\n",
    "Scroll through all supported language pairs and pick your favorite. We'll demonstrate English to Spanish, `en-to-es`\n",
    "\n",
    "Copy the link to that pair, for `en-to-es` it looks like this:\n",
    "- https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_data = 'https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-01 05:16:05--  https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip\n",
      "Resolving tico-19.github.io (tico-19.github.io)... 185.199.110.153, 185.199.109.153, 185.199.111.153, ...\n",
      "Connecting to tico-19.github.io (tico-19.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 381511 (373K) [application/zip]\n",
      "Saving to: 'all.en-es-LA.tmx.zip'\n",
      "\n",
      "all.en-es-LA.tmx.zi 100%[===================>] 372.57K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2022-07-01 05:16:05 (9.49 MB/s) - 'all.en-es-LA.tmx.zip' saved [381511/381511]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget {path_to_my_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all.en-es-LA.tmx.zip\n",
      "all.en-es-LA.tmx\n"
     ]
    }
   ],
   "source": [
    "local_file = path_to_my_data.split('/')[-1]\n",
    "print (local_file)\n",
    "filename = local_file.split('.zip')[0]\n",
    "print (filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  all.en-es-LA.tmx.zip\n",
      "  inflating: all.en-es-LA.tmx        \n"
     ]
    }
   ],
   "source": [
    "!unzip {local_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract data from `.tmx` file type \n",
    "Next, you can use this local function to extract data from the `.tmx` file type and format for local training with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the name of your file and language codes here\n",
    "source_code_1 = 'en'\n",
    "target_code_2 =  'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tmx(filename, source_code_1, target_code_2):\n",
    "    '''\n",
    "    Takes a local TMX filename and codes for source and target languages. \n",
    "    Walks through your file, row by row, looking for tmx / html specific formatting.\n",
    "    If there's a regex match, will clean your string and add to a dictionary for downstream pandas formatting.\n",
    "    '''\n",
    "    \n",
    "    data = {source_code_1:[], target_code_2:[]}\n",
    "\n",
    "    with open(filename) as f:\n",
    "\n",
    "        for row in f.readlines():\n",
    "\n",
    "            if not row.endswith('</seg></tuv>\\n'):\n",
    "                continue\n",
    "\n",
    "            if row.startswith('<seg>'):\n",
    "\n",
    "                st_1 = row.strip()\n",
    "\n",
    "                st_1 = st_1.replace('<seg>', '')\n",
    "                st_1 = st_1.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[source_code_1].append(st_1)\n",
    "\n",
    "            # when you use your own target code, remove the -LA string \n",
    "            if row.startswith('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2)):\n",
    "\n",
    "                st_2 = row.strip()\n",
    "                # when you use your own target code, remove the -LA string \n",
    "                st_2 = st_2.replace('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2), '')\n",
    "                st_2 = st_2.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[target_code_2].append(st_2)\n",
    "                \n",
    "        return data\n",
    "\n",
    "data = parse_tmx(filename, source_code_1, target_code_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes sure you got actual pairs\n",
    "assert len(data[source_code_1]) == len(data[target_code_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about how long have these symptoms been going on?</td>\n",
       "      <td>¿cuánto hace más o menos que tiene estos sínto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and all chest pain should be treated this way ...</td>\n",
       "      <td>y siempre el dolor de pecho debe tratarse de e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and along with a fever</td>\n",
       "      <td>y también fiebre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and also needs to be checked your cholesterol ...</td>\n",
       "      <td>y también debe controlarse su colesterol y pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and are you having a fever now?</td>\n",
       "      <td>¿y tiene fiebre ahora?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  about how long have these symptoms been going on?   \n",
       "1  and all chest pain should be treated this way ...   \n",
       "2                             and along with a fever   \n",
       "3  and also needs to be checked your cholesterol ...   \n",
       "4                    and are you having a fever now?   \n",
       "\n",
       "                                                  es  \n",
       "0  ¿cuánto hace más o menos que tiene estos sínto...  \n",
       "1  y siempre el dolor de pecho debe tratarse de e...  \n",
       "2                                   y también fiebre  \n",
       "3  y también debe controlarse su colesterol y pre...  \n",
       "4                             ¿y tiene fiebre ahora?  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient = 'columns')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk in case you need to restart your kernel later\n",
    "df.to_csv('language_pairs.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Format extracted data for machine translation with Hugging Face\n",
    "Core examples available right here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/translation \n",
    "\n",
    "Guidance on formatting for Hugging Face datasets here:\n",
    "https://huggingface.co/docs/datasets/loading_datasets.html#json-files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about how long have these symptoms been going on?</td>\n",
       "      <td>¿cuánto hace más o menos que tiene estos sínto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and all chest pain should be treated this way ...</td>\n",
       "      <td>y siempre el dolor de pecho debe tratarse de e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and along with a fever</td>\n",
       "      <td>y también fiebre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and also needs to be checked your cholesterol ...</td>\n",
       "      <td>y también debe controlarse su colesterol y pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and are you having a fever now?</td>\n",
       "      <td>¿y tiene fiebre ahora?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  about how long have these symptoms been going on?   \n",
       "1  and all chest pain should be treated this way ...   \n",
       "2                             and along with a fever   \n",
       "3  and also needs to be checked your cholesterol ...   \n",
       "4                    and are you having a fever now?   \n",
       "\n",
       "                                                  es  \n",
       "0  ¿cuánto hace más o menos que tiene estos sínto...  \n",
       "1  y siempre el dolor de pecho debe tratarse de e...  \n",
       "2                                   y también fiebre  \n",
       "3  y también debe controlarse su colesterol y pre...  \n",
       "4                             ¿y tiene fiebre ahora?  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('language_pairs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of translation supports only custom JSONLINES files, with each line being a dictionary with a key \"translation\" and its value another dictionary whose keys is the language pair. For example:\n",
    "\n",
    "`{ \"translation\": { \"en\": \"Others have dismissed him as a joke.\", \"ro\": \"Alții l-au numit o glumă.\" } }\n",
    "{ \"translation\": { \"en\": \"And some are holding out for an implosion.\", \"ro\": \"Iar alții așteaptă implozia.\" } }`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    obj = {\"translation\": {source_code_1: row[source_code_1], target_code_2: row[target_code_2]}} \n",
    "    objs.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation': {'en': 'about how long have these symptoms been going on?',\n",
       "   'es': '¿cuánto hace más o menos que tiene estos síntomas?'}},\n",
       " {'translation': {'en': 'and all chest pain should be treated this way especially with your age',\n",
       "   'es': 'y siempre el dolor de pecho debe tratarse de esta manera, en especial a su edad'}},\n",
       " {'translation': {'en': 'and along with a fever', 'es': 'y también fiebre'}},\n",
       " {'translation': {'en': 'and also needs to be checked your cholesterol blood pressure',\n",
       "   'es': 'y también debe controlarse su colesterol y presión arterial'}},\n",
       " {'translation': {'en': 'and are you having a fever now?',\n",
       "   'es': '¿y tiene fiebre ahora?'}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "!mkdir data\n",
    "with open('data/train.json', 'w') as f:\n",
    "    for row in objs:\n",
    "        j = json.dumps(row, ensure_ascii = False)\n",
    "        f.write(j)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Finetune a machine translation model locally\n",
    "Do to this, let's first download the raw Python file we need from Hugging Face to finetune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-01 05:16:07--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/translation/run_translation.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28254 (28K) [text/plain]\n",
      "Saving to: 'run_translation.py'\n",
      "\n",
      "run_translation.py  100%[===================>]  27.59K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2022-07-01 05:16:07 (9.90 MB/s) - 'run_translation.py' saved [28254/28254]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01/2022 05:16:09 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "07/01/2022 05:16:09 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/tst-translation/runs/Jul01_05-16-09_default,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=output/tst-translation,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/tst-translation,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "07/01/2022 05:16:10 - WARNING - datasets.builder - Using custom data configuration default-ea3ce5c4e61dec0c\n",
      "07/01/2022 05:16:10 - INFO - datasets.builder - Generating dataset json (/home/studio-lab-user/.cache/huggingface/datasets/json/default-ea3ce5c4e61dec0c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n",
      "Downloading and preparing dataset json/default to /home/studio-lab-user/.cache/huggingface/datasets/json/default-ea3ce5c4e61dec0c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 6700.17it/s]\n",
      "07/01/2022 05:16:10 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "07/01/2022 05:16:10 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 821.77it/s]\n",
      "07/01/2022 05:16:10 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "07/01/2022 05:16:10 - INFO - datasets.builder - Generating train split\n",
      "07/01/2022 05:16:10 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/studio-lab-user/.cache/huggingface/datasets/json/default-ea3ce5c4e61dec0c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 728.18it/s]\n",
      "[INFO|hub.py:592] 2022-07-01 05:16:10,289 >> https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/studio-lab-user/.cache/huggingface/transformers/tmp8it_4c8k\n",
      "Downloading: 100%|█████████████████████████| 1.17k/1.17k [00:00<00:00, 1.06MB/s]\n",
      "[INFO|hub.py:596] 2022-07-01 05:16:10,440 >> storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /home/studio-lab-user/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|hub.py:604] 2022-07-01 05:16:10,440 >> creating metadata file for /home/studio-lab-user/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:659] 2022-07-01 05:16:10,440 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:708] 2022-07-01 05:16:10,444 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:396] 2022-07-01 05:16:10,561 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:659] 2022-07-01 05:16:10,676 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:708] 2022-07-01 05:16:10,677 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|hub.py:592] 2022-07-01 05:16:10,909 >> https://huggingface.co/t5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /home/studio-lab-user/.cache/huggingface/transformers/tmp2dbt7uum\n",
      "Downloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 10.5MB/s]\n",
      "[INFO|hub.py:596] 2022-07-01 05:16:11,136 >> storing https://huggingface.co/t5-small/resolve/main/spiece.model in cache at /home/studio-lab-user/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|hub.py:604] 2022-07-01 05:16:11,137 >> creating metadata file for /home/studio-lab-user/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|hub.py:592] 2022-07-01 05:16:11,311 >> https://huggingface.co/t5-small/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/studio-lab-user/.cache/huggingface/transformers/tmpqwr649nd\n",
      "Downloading: 100%|█████████████████████████| 1.32M/1.32M [00:00<00:00, 19.7MB/s]\n",
      "[INFO|hub.py:596] 2022-07-01 05:16:11,576 >> storing https://huggingface.co/t5-small/resolve/main/tokenizer.json in cache at /home/studio-lab-user/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|hub.py:604] 2022-07-01 05:16:11,576 >> creating metadata file for /home/studio-lab-user/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-07-01 05:16:12,063 >> loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /home/studio-lab-user/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-07-01 05:16:12,064 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-07-01 05:16:12,064 >> loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-07-01 05:16:12,064 >> loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-07-01 05:16:12,064 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:659] 2022-07-01 05:16:12,214 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:708] 2022-07-01 05:16:12,215 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|hub.py:592] 2022-07-01 05:16:12,510 >> https://huggingface.co/t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/studio-lab-user/.cache/huggingface/transformers/tmpexl15bny\n",
      "Downloading: 100%|███████████████████████████| 231M/231M [00:04<00:00, 59.9MB/s]\n",
      "[INFO|hub.py:596] 2022-07-01 05:16:16,713 >> storing https://huggingface.co/t5-small/resolve/main/pytorch_model.bin in cache at /home/studio-lab-user/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|hub.py:604] 2022-07-01 05:16:16,714 >> creating metadata file for /home/studio-lab-user/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:1999] 2022-07-01 05:16:16,715 >> loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /home/studio-lab-user/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:2389] 2022-07-01 05:16:17,774 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2397] 2022-07-01 05:16:17,774 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "07/01/2022 05:16:17 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f0732fc1ca0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Running tokenizer on train dataset:   0%|                 | 0/4 [00:00<?, ?ba/s]07/01/2022 05:16:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/json/default-ea3ce5c4e61dec0c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-1c80317fa3b1799d.arrow\n",
      "Running tokenizer on train dataset: 100%|█████████| 4/4 [00:00<00:00,  6.01ba/s]\n",
      "07/01/2022 05:16:18 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.3.2/metrics/sacrebleu/sacrebleu.py not found in cache or force_download set to True, downloading to /home/studio-lab-user/.cache/huggingface/datasets/downloads/tmpz0pc1l5b\n",
      "Downloading builder script: 7.65kB [00:00, 4.08MB/s]                            \n",
      "07/01/2022 05:16:19 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.3.2/metrics/sacrebleu/sacrebleu.py in cache at /home/studio-lab-user/.cache/huggingface/datasets/downloads/f27bdf003b458d58af5858499a5451c1f2175b0090466484ea46ec595fe2c1ee.1ffdbb824af2d651e73e1b4f5816c5725c013d8f9d9a77a293308b84d8b3579a.py\n",
      "07/01/2022 05:16:19 - INFO - datasets.utils.file_utils - creating metadata file for /home/studio-lab-user/.cache/huggingface/datasets/downloads/f27bdf003b458d58af5858499a5451c1f2175b0090466484ea46ec595fe2c1ee.1ffdbb824af2d651e73e1b4f5816c5725c013d8f9d9a77a293308b84d8b3579a.py\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1521] 2022-07-01 05:16:19,069 >> ***** Running training *****\n",
      "[INFO|trainer.py:1522] 2022-07-01 05:16:19,069 >>   Num examples = 3071\n",
      "[INFO|trainer.py:1523] 2022-07-01 05:16:19,069 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1524] 2022-07-01 05:16:19,069 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1525] 2022-07-01 05:16:19,069 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1526] 2022-07-01 05:16:19,069 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1527] 2022-07-01 05:16:19,069 >>   Total optimization steps = 2304\n",
      "{'loss': 2.5983, 'learning_rate': 3.914930555555556e-05, 'epoch': 0.65}         \n",
      " 33%|█████████████▎                          | 768/2304 [15:10<24:04,  1.06it/s][INFO|trainer.py:2507] 2022-07-01 05:31:29,911 >> Saving model checkpoint to output/tst-translation/checkpoint-768\n",
      "[INFO|configuration_utils.py:446] 2022-07-01 05:31:29,912 >> Configuration saved in output/tst-translation/checkpoint-768/config.json\n",
      "[INFO|modeling_utils.py:1550] 2022-07-01 05:31:30,140 >> Model weights saved in output/tst-translation/checkpoint-768/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2143] 2022-07-01 05:31:30,141 >> tokenizer config file saved in output/tst-translation/checkpoint-768/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2150] 2022-07-01 05:31:30,141 >> Special tokens file saved in output/tst-translation/checkpoint-768/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-07-01 05:31:30,178 >> Copy vocab file to output/tst-translation/checkpoint-768/spiece.model\n",
      "{'loss': 2.2753, 'learning_rate': 2.8298611111111113e-05, 'epoch': 1.3}         \n",
      "{'loss': 2.1275, 'learning_rate': 1.7447916666666666e-05, 'epoch': 1.95}        \n",
      " 67%|██████████████████████████             | 1536/2304 [30:13<15:34,  1.22s/it][INFO|trainer.py:2507] 2022-07-01 05:46:32,539 >> Saving model checkpoint to output/tst-translation/checkpoint-1536\n",
      "[INFO|configuration_utils.py:446] 2022-07-01 05:46:32,540 >> Configuration saved in output/tst-translation/checkpoint-1536/config.json\n",
      "[INFO|modeling_utils.py:1550] 2022-07-01 05:46:32,760 >> Model weights saved in output/tst-translation/checkpoint-1536/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2143] 2022-07-01 05:46:32,761 >> tokenizer config file saved in output/tst-translation/checkpoint-1536/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2150] 2022-07-01 05:46:32,761 >> Special tokens file saved in output/tst-translation/checkpoint-1536/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-07-01 05:46:32,798 >> Copy vocab file to output/tst-translation/checkpoint-1536/spiece.model\n",
      "{'loss': 2.0552, 'learning_rate': 6.597222222222223e-06, 'epoch': 2.6}          \n",
      "100%|███████████████████████████████████████| 2304/2304 [45:08<00:00,  1.05s/it][INFO|trainer.py:2507] 2022-07-01 06:01:27,841 >> Saving model checkpoint to output/tst-translation/checkpoint-2304\n",
      "[INFO|configuration_utils.py:446] 2022-07-01 06:01:27,842 >> Configuration saved in output/tst-translation/checkpoint-2304/config.json\n",
      "[INFO|modeling_utils.py:1550] 2022-07-01 06:01:28,072 >> Model weights saved in output/tst-translation/checkpoint-2304/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2143] 2022-07-01 06:01:28,072 >> tokenizer config file saved in output/tst-translation/checkpoint-2304/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2150] 2022-07-01 06:01:28,073 >> Special tokens file saved in output/tst-translation/checkpoint-2304/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-07-01 06:01:28,112 >> Copy vocab file to output/tst-translation/checkpoint-2304/spiece.model\n",
      "[INFO|trainer.py:1766] 2022-07-01 06:01:28,561 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2709.4924, 'train_samples_per_second': 3.4, 'train_steps_per_second': 0.85, 'train_loss': 2.2346055772569446, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████| 2304/2304 [45:09<00:00,  1.18s/it]\n",
      "[INFO|trainer.py:2507] 2022-07-01 06:01:28,562 >> Saving model checkpoint to output/tst-translation\n",
      "[INFO|configuration_utils.py:446] 2022-07-01 06:01:28,563 >> Configuration saved in output/tst-translation/config.json\n",
      "[INFO|modeling_utils.py:1550] 2022-07-01 06:01:28,798 >> Model weights saved in output/tst-translation/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2143] 2022-07-01 06:01:28,799 >> tokenizer config file saved in output/tst-translation/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2150] 2022-07-01 06:01:28,799 >> Special tokens file saved in output/tst-translation/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:187] 2022-07-01 06:01:28,840 >> Copy vocab file to output/tst-translation/spiece.model\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     2.2346\n",
      "  train_runtime            = 0:45:09.49\n",
      "  train_samples            =       3071\n",
      "  train_samples_per_second =        3.4\n",
      "  train_steps_per_second   =       0.85\n",
      "[INFO|modelcard.py:460] 2022-07-01 06:01:29,003 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}}\n"
     ]
    }
   ],
   "source": [
    "# full hugging face Trainer API args available here\n",
    "# https://github.com/huggingface/transformers/blob/de635af3f1ef740aa32f53a91473269c6435e19e/src/transformers/training_args.py\n",
    "# T5 trainig args available here\n",
    "# https://huggingface.co/transformers/model_doc/t5.html#t5config\n",
    "!python run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --source_lang en \\\n",
    "    --target_lang es \\\n",
    "    --source_prefix \"translate English to Spanish: \" \\\n",
    "    --train_file data/train.json \\\n",
    "    --output_dir output/tst-translation \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --save_strategy epoch \\\n",
    "    --num_train_epochs 3\n",
    "#     --do_eval \\\n",
    "#     --validation_file path_to_jsonlines_file \\\n",
    "#     --dataset_name cov-19 \\\n",
    "#     --dataset_config_name en-es \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t  config.json\t\t   tokenizer_config.json\n",
      "all_results.json  pytorch_model.bin\t   train_results.json\n",
      "checkpoint-1536   special_tokens_map.json  trainer_state.json\n",
      "checkpoint-2304   spiece.model\t\t   training_args.bin\n",
      "checkpoint-768\t  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls output/tst-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Test your newly fine-tuned translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:993: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path = 'output/tst-translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line to make sure your model supports local inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's test it! Remember that, in using the default settings of only 3 epoch, your translation is probably not going to be SOTA. For achieving state of the art, (SOTA), we recommend migrating to Amazon SageMaker to scale up and out. Scaling up means moving your code to a more advanced compute type, such as a p4 series or even Trainium. Scaling out means adding more compute, so going from 1 to many instances. Using the entire AWS cloud you can train for much longer periods of time on much larger datasets, which can directly translate to a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about how long have these symptoms been going on? en el trabajo de sntomas?\n",
      "and all chest pain should be treated this way especially with your age\t y todos los dolores de la población del \n",
      "and along with a fever\t y a las fièvres\n",
      "and also needs to be checked your cholesterol blood pressure y es necesario a verificar la pression sanguina del\n",
      "and are you having a fever now?\t y tu ayuda una fièvre?\n",
      "and are you having any of the following symptoms with your chest pain y tiene el sntomas cio\n",
      "and are you having a runny nose? y tu ayuda un nez agua?\n",
      "and are you having this chest pain now? y tiene el dolor de la población en e\n",
      "and besides do you have difficulty breathing y ahora ahora a las dificultas\n",
      "and can you tell me what other symptoms are you having along with this? y tu pueden me dire quels sntomas\n",
      "and does this pain move from your chest? y se movió el dolor de ta pobla?\n",
      "and drink lots of fluids y boire lots de fluides\n",
      "and how high has your fever been y como el trabajo de fièvre\n",
      "and i have a cough too y es una tosa\n",
      "and i have a little cold and a cough y es un rojo y un tos\n",
      "and i'm really having some bad chest pain today y es el trabajo del traba\n"
     ]
    }
   ],
   "source": [
    "input_sequences = ['about how long have these symptoms been going on?',\t\n",
    "'and all chest pain should be treated this way especially with your age\t',\n",
    "'and along with a fever\t',\n",
    "'and also needs to be checked your cholesterol blood pressure',\t\n",
    "'and are you having a fever now?\t',\n",
    "'and are you having any of the following symptoms with your chest pain',\t\n",
    "'and are you having a runny nose?',\t\n",
    "'and are you having this chest pain now?',\n",
    "'and besides do you have difficulty breathing',\n",
    "'and can you tell me what other symptoms are you having along with this?',\n",
    "'and does this pain move from your chest?',\n",
    "'and drink lots of fluids',\n",
    "'and how high has your fever been',\n",
    "'and i have a cough too',\n",
    "'and i have a little cold and a cough',\n",
    "'''and i'm really having some bad chest pain today''']\n",
    "\n",
    "task_prefix = \"translate English to Spanish: \"\n",
    "\n",
    "for i in input_sequences:\n",
    "    input_ids = tokenizer('''{} {}'''.format(task_prefix, i), return_tensors='pt').input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    print(i, tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('my-tf-en-to-sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!tar -czf my_model.tar.gz my-tf-en-to-sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
